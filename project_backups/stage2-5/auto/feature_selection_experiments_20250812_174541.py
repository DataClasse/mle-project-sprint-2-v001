# –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢–´ –ü–û –£–õ–£–ß–®–ï–ù–ò–Æ FEATURE SELECTION
# ============================================
# –¶–µ–ª—å: –ò—Å–ø—Ä–∞–≤–∏—Ç—å —É—Ö—É–¥—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ (-3.9%) –æ—Ç Feature Selection
# –ê–≤—Ç–æ—Ä: AI Assistant
# –î–∞—Ç–∞: 2025-08-12

import pandas as pd
import yaml
import xgboost as xgb
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression, RFE, SelectFromModel
from mlxtend.feature_selection import SequentialFeatureSelector as SFS
import time
from typing import List, Tuple, Dict

# =============================================================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–•
# =============================================================================

def load_and_prepare_data() -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, List[str]]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å feature engineering"""
    print("üìä –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    with open('../config.yaml', 'r', encoding='utf-8') as f:
        config = yaml.safe_load(f)
    
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv('data/initial_data_set.csv')
    df.drop(['id', 'building_id'], axis=1, inplace=True)
    
    target = df['price']
    features = df.drop('price', axis=1)
    
    # Feature Engineering (—Ç–µ –∂–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á—Ç–æ –ø–æ–∫–∞–∑–∞–ª–∏ +1.4% —É–ª—É—á—à–µ–Ω–∏–µ)
    features_enriched = features.copy()
    
    # 1. –°–æ–æ—Ç–Ω–æ—à–µ–Ω–∏—è –ø–ª–æ—â–∞–¥–µ–π
    features_enriched['kitchen_to_total_ratio'] = features_enriched['kitchen_area'] / (features_enriched['total_area'] + 1e-8)
    features_enriched['living_to_total_ratio'] = features_enriched['living_area'] / (features_enriched['total_area'] + 1e-8)
    
    # 2. –ü—Ä–∏–∑–Ω–∞–∫–∏ —ç—Ç–∞–∂–Ω–æ—Å—Ç–∏
    features_enriched['floor_ratio'] = features_enriched['floor'] / (features_enriched['floors_total'] + 1e-8)
    
    # 3. –ü–ª–æ—â–∞–¥—å –Ω–∞ –∫–æ–º–Ω–∞—Ç—É
    features_enriched['area_per_room'] = features_enriched['total_area'] / (features_enriched['rooms'] + 1e-8)
    
    # 4. –ö–≤–∞–¥—Ä–∞—Ç—ã –∫–ª—é—á–µ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    features_enriched['total_area_sq'] = features_enriched['total_area'] ** 2
    features_enriched['ceiling_height_sq'] = features_enriched['ceiling_height'] ** 2
    
    # 5. –†–∞—Å—Å—Ç–æ—è–Ω–∏–µ –æ—Ç —Ü–µ–Ω—Ç—Ä–∞ –ú–æ—Å–∫–≤—ã
    moscow_center_lat, moscow_center_lon = 55.7558, 37.6173
    features_enriched['distance_from_center'] = np.sqrt(
        (features_enriched['latitude'] - moscow_center_lat)**2 + 
        (features_enriched['longitude'] - moscow_center_lon)**2
    )
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test = train_test_split(
        features_enriched, target, test_size=0.2, random_state=42, shuffle=True
    )
    
    feature_names = features_enriched.columns.tolist()
    
    print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω—ã:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_train.shape}")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {X_test.shape}")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(feature_names)}")
    print()
    
    return X_train, X_test, y_train, y_test, feature_names

def evaluate_model(X_train, X_test, y_train, y_test, selected_features: List[str] = None) -> Dict:
    """–û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞ –º–æ–¥–µ–ª–∏"""
    if selected_features:
        X_train_eval = X_train[selected_features]
        X_test_eval = X_test[selected_features]
    else:
        X_train_eval = X_train
        X_test_eval = X_test
    
    # –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏
    model = xgb.XGBRegressor(
        n_estimators=800,
        max_depth=6,
        learning_rate=0.1,
        random_state=42,
        n_jobs=-1
    )
    
    model.fit(X_train_eval, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred = model.predict(X_test_eval)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    rmse = mean_squared_error(y_test, y_pred, squared=False)
    r2 = r2_score(y_test, y_pred)
    mae = mean_absolute_error(y_test, y_pred)
    
    # Cross-validation
    cv_scores = cross_val_score(
        model, X_train_eval, y_train, 
        cv=3, scoring='neg_root_mean_squared_error'
    )
    cv_rmse = -cv_scores.mean()
    
    return {
        'rmse': rmse,
        'r2': r2,
        'mae': mae,
        'cv_rmse': cv_rmse,
        'n_features': X_train_eval.shape[1]
    }

# =============================================================================
# BASELINE (–ë–ï–ó FEATURE SELECTION)
# =============================================================================

def run_baseline_experiment(X_train, X_test, y_train, y_test) -> Dict:
    """Baseline: –≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –±–µ–∑ –æ—Ç–±–æ—Ä–∞"""
    print("üèÅ BASELINE: –í—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–±–µ–∑ Feature Selection)")
    print("-" * 50)
    
    start_time = time.time()
    results = evaluate_model(X_train, X_test, y_train, y_test)
    duration = time.time() - start_time
    
    print(f"‚úÖ Baseline —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã ({duration:.1f}—Å):")
    print(f"   RMSE: {results['rmse']:,.0f}")
    print(f"   R¬≤: {results['r2']:.4f}")
    print(f"   MAE: {results['mae']:,.0f}")
    print(f"   CV RMSE: {results['cv_rmse']:,.0f}")
    print(f"   –ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {results['n_features']}")
    print()
    
    return results

# =============================================================================
# –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: –£–õ–£–ß–®–ï–ù–ù–´–ô SEQUENTIAL FEATURE SELECTION
# =============================================================================

def run_sfs_experiment(X_train, X_test, y_train, y_test, feature_names: List[str]) -> Dict:
    """–≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç 1: Sequential Feature Selection —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏"""
    print("üî¨ –≠–ö–°–ü–ï–†–ò–ú–ï–ù–¢ 1: –£–ª—É—á—à–µ–Ω–Ω—ã–π Sequential Feature Selection")
    print("-" * 60)
    
    start_time = time.time()
    
    # –ë–æ–ª–µ–µ –º–æ—â–Ω–∞—è –±–∞–∑–æ–≤–∞—è –º–æ–¥–µ–ª—å –¥–ª—è SFS
    base_model = xgb.XGBRegressor(
        n_estimators=300,  # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ SFS
        max_depth=6,
        learning_rate=0.1,
        subsample=0.9,
        colsample_bytree=0.8,
        random_state=42,
        n_jobs=1  # –î–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏ SFS
    )
    
    # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    best_results = None
    best_features = None
    best_k = None
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Ä–∞–∑–Ω—ã–µ k –æ—Ç 10 –¥–æ 18 (–±—ã–ª–æ 12)
    for k in [10, 12, 14, 16, 18]:
        print(f"üîç –¢–µ—Å—Ç–∏—Ä—É–µ–º k={k} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤...")
        
        # Sequential Forward Selection
        sfs = SFS(
            base_model,
            k_features=k,
            forward=True,
            floating=False,
            scoring='neg_root_mean_squared_error',
            cv=3,  # –£–º–µ–Ω—å—à–∏–ª–∏ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
            n_jobs=1
        )
        
        try:
            sfs.fit(X_train, y_train)
            selected_features = list(sfs.k_feature_names_)
            
            # –û—Ü–µ–Ω–∫–∞ –Ω–∞ –ø–æ–ª–Ω–æ–π –º–æ–¥–µ–ª–∏
            results = evaluate_model(X_train, X_test, y_train, y_test, selected_features)
            
            print(f"   k={k}: RMSE={results['rmse']:,.0f}, R¬≤={results['r2']:.4f}")
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ª—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            if best_results is None or results['rmse'] < best_results['rmse']:
                best_results = results
                best_features = selected_features
                best_k = k
                
        except Exception as e:
            print(f"   k={k}: –û—à–∏–±–∫–∞ - {str(e)[:50]}...")
    
    duration = time.time() - start_time
    
    print(f"‚úÖ –õ—É—á—à–∏–π SFS —Ä–µ–∑—É–ª—å—Ç–∞—Ç ({duration:.1f}—Å):")
    print(f"   –õ—É—á—à–∏–π k: {best_k}")
    print(f"   RMSE: {best_results['rmse']:,.0f}")
    print(f"   R¬≤: {best_results['r2']:.4f}")
    print(f"   MAE: {best_results['mae']:,.0f}")
    print(f"   CV RMSE: {best_results['cv_rmse']:,.0f}")
    print(f"   –í—ã–±—Ä–∞–Ω–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ ({len(best_features)}):")
    for i, feature in enumerate(best_features[:8]):
        print(f"     {i+1}. {feature}")
    if len(best_features) > 8:
        print(f"     ... –∏ –µ—â–µ {len(best_features) - 8}")
    print()
    
    return {**best_results, 'selected_features': best_features, 'method': 'SFS_improved'}

if __name__ == "__main__":
    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    X_train, X_test, y_train, y_test, feature_names = load_and_prepare_data()
    
    # –ó–∞–ø—É—Å–∫ baseline
    baseline_results = run_baseline_experiment(X_train, X_test, y_train, y_test)
    
    # –ó–∞–ø—É—Å–∫ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ 1
    sfs_results = run_sfs_experiment(X_train, X_test, y_train, y_test, feature_names)
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í:")
    print("=" * 40)
    print(f"Baseline (–≤—Å–µ –ø—Ä–∏–∑–Ω–∞–∫–∏): RMSE={baseline_results['rmse']:,.0f}, R¬≤={baseline_results['r2']:.4f}")
    print(f"SFS —É–ª—É—á—à–µ–Ω–Ω—ã–π: RMSE={sfs_results['rmse']:,.0f}, R¬≤={sfs_results['r2']:.4f}")
    
    improvement = ((baseline_results['rmse'] - sfs_results['rmse']) / baseline_results['rmse']) * 100
    print(f"–£–ª—É—á—à–µ–Ω–∏–µ: {improvement:+.1f}%")
    
    if improvement > 0:
        print("‚úÖ SFS –ø–æ–∫–∞–∑–∞–ª —É–ª—É—á—à–µ–Ω–∏–µ!")
    else:
        print("‚ö†Ô∏è SFS –≤—Å–µ –µ—â–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç —É—Ö—É–¥—à–µ–Ω–∏–µ")

