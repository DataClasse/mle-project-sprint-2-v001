#!/usr/bin/env python3
"""
–°—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤–ª–∏—è–Ω–∏—è –∏—Å—Ö–æ–¥–Ω—ã—Ö –∏ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
–ê–Ω–∞–ª–∏–∑ –¥–ª—è –ø—Ä–∏–Ω—è—Ç–∏—è —Ä–µ—à–µ–Ω–∏—è –æ –∑–∞–º–µ–Ω–µ –¥–∞—Ç–∞—Å–µ—Ç–∞
"""

import pandas as pd
import numpy as np
import yaml
import json
import os
from pathlib import Path
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
import xgboost as xgb
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

def load_config():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ config.yaml"""
    config_path = "../config.yaml"
    with open(config_path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)

def analyze_dataset(df, name):
    """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∏ –∫–∞—á–µ—Å—Ç–≤–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print(f"\nüîç –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê: {name}")
    print("=" * 50)
    
    # –ë–∞–∑–æ–≤–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
    print(f"üìä –†–∞–∑–º–µ—Ä: {df.shape[0]:,} —Å—Ç—Ä–æ–∫ √ó {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
    print(f"üíæ –ü–∞–º—è—Ç—å: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB")
    
    # –¢–∏–ø—ã –¥–∞–Ω–Ω—ã—Ö
    print(f"\nüìã –¢–ò–ü–´ –î–ê–ù–ù–´–•:")
    data_types = df.dtypes.value_counts()
    for dtype, count in data_types.items():
        print(f"   {dtype}: {count} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    # –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    missing = df.isnull().sum()
    missing_pct = (missing / len(df)) * 100
    missing_info = missing[missing > 0]
    
    if len(missing_info) > 0:
        print(f"\n‚ùå –ü–†–û–ü–£–©–ï–ù–ù–´–ï –ó–ù–ê–ß–ï–ù–ò–Ø:")
        for col, count in missing_info.items():
            print(f"   {col}: {count:,} ({missing_pct[col]:.1f}%)")
    else:
        print(f"\n‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")
    
    # –ß–∏—Å–ª–æ–≤—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    if len(numeric_cols) > 0:
        print(f"\nüìà –ß–ò–°–õ–û–í–´–ï –°–¢–û–õ–ë–¶–´: {len(numeric_cols)}")
        numeric_stats = df[numeric_cols].describe()
        print("   –ú–∏–Ω/–ú–∞–∫—Å –∑–Ω–∞—á–µ–Ω–∏—è:")
        for col in numeric_cols[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            min_val = numeric_stats.loc['min', col]
            max_val = numeric_stats.loc['max', col]
            print(f"   {col}: {min_val:.2f} ‚Äî {max_val:.2f}")
        if len(numeric_cols) > 5:
            print(f"   ... –∏ –µ—â—ë {len(numeric_cols) - 5} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã
    cat_cols = df.select_dtypes(include=['object']).columns
    if len(cat_cols) > 0:
        print(f"\nüìù –ö–ê–¢–ï–ì–û–†–ò–ê–õ–¨–ù–´–ï –°–¢–û–õ–ë–¶–´: {len(cat_cols)}")
        for col in cat_cols[:3]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 3
            unique_count = df[col].nunique()
            print(f"   {col}: {unique_count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
        if len(cat_cols) > 3:
            print(f"   ... –∏ –µ—â—ë {len(cat_cols) - 3} —Å—Ç–æ–ª–±—Ü–æ–≤")
    
    return {
        'shape': df.shape,
        'memory_mb': df.memory_usage(deep=True).sum() / 1024**2,
        'missing_count': len(missing_info),
        'missing_total': missing.sum(),
        'numeric_cols': len(numeric_cols),
        'categorical_cols': len(cat_cols),
        'columns': list(df.columns)
    }

def prepare_data_for_ml(df, config, target_col):
    """–ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"""
    # –£–±–∏—Ä–∞–µ–º target –∏–∑ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    feature_cols = [col for col in df.columns if col != target_col]
    
    X = df[feature_cols].copy()
    y = df[target_col].copy()
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ
    categorical_cols = X.select_dtypes(include=['object']).columns
    
    if len(categorical_cols) > 0:
        print(f"üìù –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º {len(categorical_cols)} –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤...")
        from sklearn.preprocessing import LabelEncoder
        
        for col in categorical_cols:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    if X.isnull().sum().sum() > 0:
        print("üîß –ó–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –º–µ–¥–∏–∞–Ω–æ–π...")
        X = X.fillna(X.median())
    
    return X, y

def train_and_evaluate_model(X, y, name, config):
    """–û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏"""
    print(f"\nü§ñ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ò –ù–ê: {name}")
    print("=" * 50)
    
    # –†–∞–∑–¥–µ–ª—è–µ–º –¥–∞–Ω–Ω—ã–µ
    test_size = config.get('model', {}).get('test_size', 0.2)
    random_state = config.get('model', {}).get('random_state', 42)
    
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, 
        test_size=test_size, 
        random_state=random_state
    )
    
    print(f"üìä –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {X_train.shape[0]:,}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {X_test.shape[0]:,}")
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    model_params = config.get('model', {}).get('params', {})
    model_params['random_state'] = random_state
    
    # –°–æ–∑–¥–∞–µ–º –∏ –æ–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    model = xgb.XGBRegressor(**model_params)
    
    print("‚è≥ –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
    model.fit(X_train, y_train)
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    y_pred_train = model.predict(X_train)
    y_pred_test = model.predict(X_test)
    
    # –ú–µ—Ç—Ä–∏–∫–∏
    metrics = {
        'train_mse': mean_squared_error(y_train, y_pred_train),
        'test_mse': mean_squared_error(y_test, y_pred_test),
        'train_rmse': np.sqrt(mean_squared_error(y_train, y_pred_train)),
        'test_rmse': np.sqrt(mean_squared_error(y_test, y_pred_test)),
        'train_mae': mean_absolute_error(y_train, y_pred_train),
        'test_mae': mean_absolute_error(y_test, y_pred_test),
        'train_r2': r2_score(y_train, y_pred_train),
        'test_r2': r2_score(y_test, y_pred_test)
    }
    
    # Cross-validation
    print("üîÑ Cross-validation...")
    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
    metrics['cv_rmse_mean'] = np.sqrt(-cv_scores.mean())
    metrics['cv_rmse_std'] = np.sqrt(cv_scores.std())
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    print(f"\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´:")
    print(f"   Train RMSE: {metrics['train_rmse']:.4f}")
    print(f"   Test RMSE:  {metrics['test_rmse']:.4f}")
    print(f"   Train R¬≤:   {metrics['train_r2']:.4f}")
    print(f"   Test R¬≤:    {metrics['test_r2']:.4f}")
    print(f"   CV RMSE:    {metrics['cv_rmse_mean']:.4f} ¬± {metrics['cv_rmse_std']:.4f}")
    
    # –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ
    overfit = metrics['train_rmse'] - metrics['test_rmse']
    print(f"   –ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ: {overfit:.4f} ({'üü¢ –ù–∏–∑–∫–æ–µ' if abs(overfit) < 0.1 else 'üü° –°—Ä–µ–¥–Ω–µ–µ' if abs(overfit) < 0.3 else 'üî¥ –í—ã—Å–æ–∫–æ–µ'})")
    
    return model, metrics

def compare_metrics(metrics1, metrics2, name1, name2):
    """–°—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–≤—É—Ö –º–æ–¥–µ–ª–µ–π"""
    print(f"\nüÜö –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("=" * 50)
    
    key_metrics = ['test_rmse', 'test_r2', 'cv_rmse_mean']
    
    print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<15} {name1:<15} {name2:<15} {'–£–ª—É—á—à–µ–Ω–∏–µ':<15}")
    print("-" * 65)
    
    improvements = {}
    
    for metric in key_metrics:
        val1 = metrics1[metric]
        val2 = metrics2[metric]
        
        if metric in ['test_rmse', 'cv_rmse_mean']:
            # –î–ª—è RMSE - –º–µ–Ω—å—à–µ –ª—É—á—à–µ
            improvement = ((val1 - val2) / val1) * 100
            improvement_text = f"{improvement:+.2f}%"
            is_better = val2 < val1
        else:
            # –î–ª—è R¬≤ - –±–æ–ª—å—à–µ –ª—É—á—à–µ
            improvement = ((val2 - val1) / abs(val1)) * 100
            improvement_text = f"{improvement:+.2f}%"
            is_better = val2 > val1
        
        improvements[metric] = improvement
        
        status = "üü¢" if is_better else "üî¥"
        print(f"{metric:<15} {val1:<15.4f} {val2:<15.4f} {status} {improvement_text:<10}")
    
    return improvements

def generate_recommendation(dataset_stats1, dataset_stats2, improvements, name1, name2):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é –ø–æ –∑–∞–º–µ–Ω–µ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print(f"\nüéØ –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–Ø –ü–û –ó–ê–ú–ï–ù–ï –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 50)
    
    # –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
    data_quality_score = 0
    
    # –†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞
    size_ratio = dataset_stats2['shape'][0] / dataset_stats1['shape'][0]
    if size_ratio > 1.1:
        print(f"‚úÖ –†–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: —É–≤–µ–ª–∏—á–µ–Ω–∏–µ –Ω–∞ {(size_ratio-1)*100:.1f}% ({dataset_stats2['shape'][0]:,} vs {dataset_stats1['shape'][0]:,})")
        data_quality_score += 1
    
    # –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (–º–µ–Ω—å—à–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤)
    if dataset_stats2['missing_total'] < dataset_stats1['missing_total']:
        print(f"‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: –º–µ–Ω—å—à–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ ({dataset_stats2['missing_total']} vs {dataset_stats1['missing_total']})")
        data_quality_score += 1
    
    # –ê–Ω–∞–ª–∏–∑ –º–µ—Ç—Ä–∏–∫
    metric_improvements = 0
    key_metrics = ['test_rmse', 'test_r2', 'cv_rmse_mean']
    
    for metric in key_metrics:
        if metric in ['test_rmse', 'cv_rmse_mean']:
            if improvements[metric] > 0:  # –£–º–µ–Ω—å—à–µ–Ω–∏–µ RMSE - —Ö–æ—Ä–æ—à–æ
                metric_improvements += 1
        else:
            if improvements[metric] > 0:  # –£–≤–µ–ª–∏—á–µ–Ω–∏–µ R¬≤ - —Ö–æ—Ä–æ—à–æ
                metric_improvements += 1
    
    print(f"\nüìä –û–¶–ï–ù–ö–ê –£–õ–£–ß–®–ï–ù–ò–ô:")
    print(f"   –ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö: {data_quality_score}/2")
    print(f"   –£–ª—É—á—à–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫: {metric_improvements}/{len(key_metrics)}")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è
    total_score = data_quality_score + metric_improvements
    max_score = 2 + len(key_metrics)
    
    if total_score >= max_score * 0.7:
        recommendation = "üü¢ –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –ó–ê–ú–ï–ù–ê"
        reason = "–£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"
    elif total_score >= max_score * 0.4:
        recommendation = "üü° –†–ê–°–°–ú–û–¢–†–ï–¢–¨ –ó–ê–ú–ï–ù–£"
        reason = "–ï—Å—Ç—å —É–ª—É—á—à–µ–Ω–∏—è, –Ω–æ –æ–Ω–∏ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω—ã–µ"
    else:
        recommendation = "üî¥ –ù–ï –†–ï–ö–û–ú–ï–ù–î–£–ï–¢–°–Ø –ó–ê–ú–ï–ù–ê"
        reason = "–£–ª—É—á—à–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤"
    
    print(f"\n{recommendation}")
    print(f"–û–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ: {reason}")
    print(f"–û–±—â–∏–π –±–∞–ª–ª: {total_score}/{max_score}")
    
    return recommendation, total_score, max_score

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è"""
    print("üîç –ê–ù–ê–õ–ò–ó –í–õ–ò–Ø–ù–ò–Ø –ó–ê–ú–ï–ù–´ –î–ê–¢–ê–°–ï–¢–ê –ù–ê –ú–û–î–ï–õ–¨")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_config()
    target_col = config['preprocessing']['features']['target_column']
    
    # –ü—É—Ç–∏ –∫ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    dataset1_path = "data/initial_data_set.csv"
    dataset2_path = "data/merged_data_improved.csv"
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤...")
    df1 = pd.read_csv(dataset1_path)
    df2 = pd.read_csv(dataset2_path)
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö
    stats1 = analyze_dataset(df1, "–ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï")
    stats2 = analyze_dataset(df2, "–£–õ–£–ß–®–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π
    if target_col not in df1.columns:
        raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{target_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    if target_col not in df2.columns:
        raise ValueError(f"–¶–µ–ª–µ–≤–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è '{target_col}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ —É–ª—É—á—à–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è ML
    print(f"\nüîß –ü–û–î–ì–û–¢–û–í–ö–ê –î–ê–ù–ù–´–• –î–õ–Ø ML (target: {target_col})")
    X1, y1 = prepare_data_for_ml(df1, config, target_col)
    X2, y2 = prepare_data_for_ml(df2, config, target_col)
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏ –∏ –ø–æ–ª—É—á–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    model1, metrics1 = train_and_evaluate_model(X1, y1, "–ò–°–•–û–î–ù–´–ï –î–ê–ù–ù–´–ï", config)
    model2, metrics2 = train_and_evaluate_model(X2, y2, "–£–õ–£–ß–®–ï–ù–ù–´–ï –î–ê–ù–ù–´–ï", config)
    
    # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    improvements = compare_metrics(metrics1, metrics2, "–ò—Å—Ö–æ–¥–Ω—ã–µ", "–£–ª—É—á—à–µ–Ω–Ω—ã–µ")
    
    # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—é
    recommendation, score, max_score = generate_recommendation(
        stats1, stats2, improvements, "–∏—Å—Ö–æ–¥–Ω—ã—Ö", "—É–ª—É—á—à–µ–Ω–Ω—ã—Ö"
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    results = {
        'timestamp': timestamp,
        'datasets': {
            'original': {
                'path': dataset1_path,
                'stats': stats1
            },
            'improved': {
                'path': dataset2_path,
                'stats': stats2
            }
        },
        'metrics': {
            'original': metrics1,
            'improved': metrics2,
            'improvements': improvements
        },
        'recommendation': {
            'decision': recommendation,
            'score': f"{score}/{max_score}",
            'timestamp': timestamp
        }
    }
    
    results_path = f"cv_results/dataset_comparison_{timestamp}.json"
    os.makedirs("cv_results", exist_ok=True)
    
    with open(results_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
    
    return recommendation, results

if __name__ == "__main__":
    try:
        recommendation, results = main()
        print(f"\nüéâ –ê–ù–ê–õ–ò–ó –ó–ê–í–ï–†–®–ï–ù: {recommendation}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}")
        raise
